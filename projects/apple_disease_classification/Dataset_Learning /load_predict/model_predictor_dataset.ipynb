{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70995b31-0628-4bd1-a67d-1ecba9ba1402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image# Helper libraries\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d355089-cb85-48fd-a949-9bfbee1c3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dafb247a-c29c-4f67-b8a4-065fd5429685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image dataset\n",
    "dataset_path = \"../../data/80apples/\"\n",
    "data_dir = pathlib.Path(dataset_path)\n",
    "apple_images = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24cefae9-b162-4c21-83a9-8367fd51b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(dataset_path):\n",
    "    imgAppleDirectory = os.path.join(dataset_path)\n",
    "    apple_images.append(imgAppleDirectory)\n",
    "    # print(apple_images)\n",
    "    \n",
    "apple_images = [img for img in apple_images if \".jpg\" in img]\n",
    "# print(type(apple_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "346f5b8e-6b7d-44d2-bdc7-88e817139439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = image.load_img(apple_images, target_size=(224, 224))\n",
    "img_size = (224, 224)\n",
    "batch_size= 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a38e02c2-4b02-4708-862f-d64afe63cd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "apple_images = tf.keras.utils.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  image_size=img_size,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49411234-cbdc-4c3d-a53c-7fd50d7c88c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1541786546.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [72], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    # print(element)\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# for element in apple_images:\n",
    "  # print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e04e0e11-da79-431a-a771-cf4976524515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 224, 224, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.math.subtract (TFOpLambd  (None, 224, 224, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 5124      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,263,108\n",
      "Trainable params: 1,866,564\n",
      "Non-trainable params: 396,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Loadmodel\n",
    "model = tf.keras.models.load_model('../saved_models/tl_mobileNetV2/model/tl_22112022_12h10_1.h5', custom_objects=None, compile=True, options=None)\n",
    "classifier = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\", input_shape=img_size+(3,))\n",
    "])\n",
    "# Show the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5411a56-75ec-4222-84eb-68c251a7d28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1e5615a9-e0d6-4c84-8624-323b0867ed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d2473602-b102-45cb-860d-fea57cf64c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 3s 398ms/step\n"
     ]
    }
   ],
   "source": [
    "# batchPredictions = model.predict(apple_images)\n",
    "# predicted_categories = tf.argmax(batchPredictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e934c1e9-dfc3-4adf-8acf-4361ddf0d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for element in apple_images:\n",
    "#   # print(element)\n",
    "#     img_array = image.img_to_array(apple_images)\n",
    "#     img_batch = np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97a879b1-4df7-4dac-8947-172674e79e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_preprocessed = preprocess_input(apple_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb94c764-67bd-42a1-8c3e-bc286bc0cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = model.predict(img_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1bdedaf4-92fb-4341-8ef1-4f8300b6916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_preprocessed[np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3dab9d11-58b7-4bde-8994-207fde3fc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_label_index = np.argmax(img_preprocessed)\n",
    "# predicted_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6223c2f0-f87e-45f9-967b-ce08f3cf1cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 2s 408ms/step\n",
      "Predictions:\n",
      " 25\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(apple_images).flatten()\n",
    "# print(predictions)\n",
    "\n",
    "predictions = tf.nn.softmax(predictions)\n",
    "# predictions = tf.where(predictions < 0.1, 0, 1)\n",
    "predictions = tf.math.argmax(predictions)\n",
    "\n",
    "print('Predictions:\\n', predictions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198a1b02-38b3-4a68-91d1-1339160e4e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
